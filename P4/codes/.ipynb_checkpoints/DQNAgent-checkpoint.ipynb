{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/daiane/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys, time, random, math, cv2, os, copy\n",
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "from robot import Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cicle(ang):\n",
    "    return (ang + 2 * math.pi) % (2*math.pi)\n",
    "\n",
    "def get_distance_from_goal(a,b):\n",
    "    return np.linalg.norm(a-b)\n",
    "\n",
    "def get_angular_distance(v1,v2):\n",
    "\n",
    "    v1_aux = v1 / np.linalg.norm(v1)\n",
    "    v2_aux = v2 / np.linalg.norm(v2)\n",
    "\n",
    "    theta = np.arccos(np.clip(np.dot(v1_aux, v2_aux), -1.0, 1.0))\n",
    "    rot_theta = np.array([[np.cos(theta),-np.sin(theta)],[np.sin(theta),np.cos(theta)]])\n",
    "    theta = theta*180.0/math.pi\n",
    "    theta = (360.0+theta)%360.0\n",
    "\n",
    "    if abs(np.dot(np.dot(rot_theta,v2_aux),v1_aux)-1.0) > 1e-2:\n",
    "        theta = 360.0 - theta\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvRobot(): \n",
    "    def __init__(self, robot, goal):\n",
    "        self.robot = robot\n",
    "        self.goal = goal\n",
    "        self.random, _ = seeding.np_random(12345)\n",
    "        self.apply_observation()\n",
    "        self.actions = [[2,0.5],[0.5,2],[2,2]]\n",
    "        \n",
    "    def apply_observation(self):\n",
    "        current_position = np.array(self.robot.get_current_position()[:-1])\n",
    "        current_angle = cicle(self.robot.get_current_orientation()[2])\n",
    "\n",
    "        posRel = np.array(self.goal) - np.array(current_position)\n",
    "        self.observation = np.array([posRel[0], posRel[1], current_angle])\n",
    "    \n",
    "    def apply_action(self, a):\n",
    "        action = self.actions[a]\n",
    "        self.robot.set_left_velocity(action[0])\n",
    "        self.robot.set_right_velocity(action[1])\n",
    "        \n",
    "    def step(self, action):\n",
    "        old_position = np.array(self.robot.get_current_position()[:-1])\n",
    "        old_angle = cicle(self.robot.get_current_orientation()[2])\n",
    "        \n",
    "        self.apply_action(action)\n",
    "        self.robot.step_simulation()\n",
    "        obsAnterior = copy.copy(self.observation)\n",
    "        self.apply_observation()\n",
    "        \n",
    "        new_position = np.array(self.robot.get_current_position()[:-1])\n",
    "        new_angle = cicle(self.robot.get_current_orientation()[2])\n",
    "        \n",
    "        distance_old = get_distance_from_goal(old_position, self.goal)\n",
    "        distance_new = get_distance_from_goal(new_position, self.goal)\n",
    "        \n",
    "        angle_erro_old = get_angular_distance(self.goal - old_position, np.array([math.cos(old_angle),math.sin(old_angle)])) # graus\n",
    "        angle_erro_new = get_angular_distance(self.goal - new_position, np.array([math.cos(new_angle),math.sin(new_angle)])) # graus\n",
    "    \n",
    "        us_distances = robot.read_ultrassonic_sensors()\n",
    "        flag = False\n",
    "        for i in us_distances[:8]:\n",
    "            flag = flag or (i < 0.7)\n",
    "        \n",
    "        reward = (distance_old - distance_new) + ( (angle_erro_old/180) - (angle_erro_new/180) ) - flag*2\n",
    "        done = distance_new < 0.2\n",
    "            \n",
    "        return self.observation, reward, done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.robot.get_connection_status() != -1:\n",
    "            positions = [[11.5,0.4,0.1],[11.3,0.5,0.1],[10.15,0.3,0.1]] # positions begin\n",
    "            pos = np.random.randint(len(positions))\n",
    "            tupla = [ [], positions[pos], [], bytearray() ]\n",
    "            self.robot.call_childscript_function(tupla)\n",
    "        else:\n",
    "            self.robot.start_simulation()\n",
    "        \n",
    "        self.apply_observation()\n",
    "        return self.observation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to remoteApi server.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor1 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor2 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor3 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor4 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor5 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor6 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor7 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor8 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor9 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor10 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor11 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor12 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor13 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor14 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor15 connected.\n",
      "\u001b[92m Pioneer_p3dx_ultrasonicSensor16 connected.\n",
      "\u001b[92m Vision sensor connected.\n",
      "\u001b[92m Laser connected.\n",
      "\u001b[92m Left motor connected.\n",
      "\u001b[92m Right motor connected.\n",
      "\u001b[92m Robot connected.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 2,339\n",
      "Trainable params: 2,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Carregando modelo...\n",
      "Training for 2000 steps ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "  100/2000: episode: 1, duration: 21.911s, episode steps: 100, steps per second: 5, episode reward: 2.648, mean reward: 0.026 [-0.001, 1.964], mean action: 1.030 [0.000, 2.000], mean observation: -6.563 [-23.375, 3.537], loss: 0.025014, acc: 0.371735, mean_q: 3.261087\n",
      "  200/2000: episode: 2, duration: 13.554s, episode steps: 100, steps per second: 7, episode reward: 0.743, mean reward: 0.007 [-0.001, 0.014], mean action: 0.960 [0.000, 2.000], mean observation: -6.432 [-22.478, 3.148], loss: 0.040660, acc: 0.312500, mean_q: 3.013364\n",
      "  300/2000: episode: 3, duration: 14.877s, episode steps: 100, steps per second: 7, episode reward: -1.058, mean reward: -0.011 [-1.948, 0.013], mean action: 1.110 [0.000, 2.000], mean observation: -6.124 [-21.754, 3.272], loss: 0.037884, acc: 0.312187, mean_q: 2.268440\n",
      "  400/2000: episode: 4, duration: 15.275s, episode steps: 100, steps per second: 7, episode reward: 2.766, mean reward: 0.028 [-1.951, 1.965], mean action: 0.860 [0.000, 2.000], mean observation: -5.796 [-20.884, 3.280], loss: 0.037917, acc: 0.330312, mean_q: 1.929468\n",
      "  500/2000: episode: 5, duration: 13.677s, episode steps: 100, steps per second: 7, episode reward: -163.268, mean reward: -1.633 [-1.999, 0.011], mean action: 1.000 [0.000, 2.000], mean observation: -5.620 [-20.072, 3.108], loss: 0.163461, acc: 0.309687, mean_q: 1.634814\n",
      "  600/2000: episode: 6, duration: 13.491s, episode steps: 100, steps per second: 7, episode reward: -79.239, mean reward: -0.792 [-1.997, 0.011], mean action: 1.000 [0.000, 2.000], mean observation: -5.422 [-19.247, 3.065], loss: 0.319516, acc: 0.290625, mean_q: 0.744409\n",
      "  700/2000: episode: 7, duration: 13.126s, episode steps: 100, steps per second: 8, episode reward: 0.390, mean reward: 0.004 [-0.003, 0.009], mean action: 1.010 [0.000, 2.000], mean observation: -5.476 [-18.435, 2.752], loss: 0.289246, acc: 0.328750, mean_q: -0.049242\n",
      "  800/2000: episode: 8, duration: 12.962s, episode steps: 100, steps per second: 8, episode reward: 0.470, mean reward: 0.005 [-0.003, 0.010], mean action: 0.990 [0.000, 2.000], mean observation: -5.505 [-17.856, 2.491], loss: 0.265796, acc: 0.323438, mean_q: -0.658358\n",
      "  900/2000: episode: 9, duration: 12.952s, episode steps: 100, steps per second: 8, episode reward: 0.305, mean reward: 0.003 [-0.003, 0.007], mean action: 1.100 [0.000, 2.000], mean observation: -5.650 [-17.301, 2.220], loss: 0.232066, acc: 0.363125, mean_q: -1.139086\n",
      " 1000/2000: episode: 10, duration: 12.604s, episode steps: 100, steps per second: 8, episode reward: 0.265, mean reward: 0.003 [-0.004, 0.008], mean action: 0.990 [0.000, 2.000], mean observation: -5.756 [-16.887, 2.188], loss: 0.228504, acc: 0.400625, mean_q: -1.631803\n",
      " 1100/2000: episode: 11, duration: 13.134s, episode steps: 100, steps per second: 8, episode reward: -163.653, mean reward: -1.637 [-2.004, 0.008], mean action: 0.880 [0.000, 2.000], mean observation: -5.750 [-16.481, 2.447], loss: 0.267036, acc: 0.413750, mean_q: -2.136361\n",
      " 1200/2000: episode: 12, duration: 13.359s, episode steps: 100, steps per second: 7, episode reward: -199.965, mean reward: -2.000 [-2.000, -1.997], mean action: 0.920 [0.000, 2.000], mean observation: -5.835 [-15.968, 2.122], loss: 0.266280, acc: 0.350000, mean_q: -2.858622\n",
      " 1300/2000: episode: 13, duration: 13.294s, episode steps: 100, steps per second: 8, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 0.940 [0.000, 2.000], mean observation: -5.836 [-15.911, 2.113], loss: 0.270782, acc: 0.347813, mean_q: -3.769022\n",
      " 1400/2000: episode: 14, duration: 13.530s, episode steps: 100, steps per second: 7, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 1.220 [0.000, 2.000], mean observation: -5.836 [-15.910, 2.113], loss: 0.430213, acc: 0.332188, mean_q: -4.672131\n",
      " 1500/2000: episode: 15, duration: 13.356s, episode steps: 100, steps per second: 7, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 0.970 [0.000, 2.000], mean observation: -5.836 [-15.910, 2.113], loss: 0.414485, acc: 0.387188, mean_q: -5.891140\n",
      " 1600/2000: episode: 16, duration: 12.913s, episode steps: 100, steps per second: 8, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 1.070 [0.000, 2.000], mean observation: -5.836 [-15.910, 2.113], loss: 0.696821, acc: 0.368125, mean_q: -6.727506\n",
      " 1700/2000: episode: 17, duration: 12.951s, episode steps: 100, steps per second: 8, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 0.900 [0.000, 2.000], mean observation: -5.836 [-15.909, 2.112], loss: 0.517826, acc: 0.385938, mean_q: -8.127471\n",
      " 1800/2000: episode: 18, duration: 13.881s, episode steps: 100, steps per second: 7, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 1.170 [0.000, 2.000], mean observation: -5.836 [-15.909, 2.112], loss: 0.753942, acc: 0.366562, mean_q: -9.196546\n",
      " 1900/2000: episode: 19, duration: 13.506s, episode steps: 100, steps per second: 7, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 0.960 [0.000, 2.000], mean observation: -5.836 [-15.909, 2.112], loss: 0.992408, acc: 0.504375, mean_q: -10.376078\n",
      " 2000/2000: episode: 20, duration: 13.024s, episode steps: 100, steps per second: 8, episode reward: -200.000, mean reward: -2.000 [-2.000, -2.000], mean action: 1.090 [0.000, 2.000], mean observation: -5.835 [-15.908, 2.113], loss: 1.212719, acc: 0.399687, mean_q: -11.310334\n",
      "done, took 277.412 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = 'dqn_weights.h5f'\n",
    "robot = Robot(\"\")\n",
    "goal = np.array([-12,0])\n",
    "\n",
    "env = EnvRobot(robot, goal)\n",
    "\n",
    "n_actions = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + (n_actions,)))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "memory = SequentialMemory(limit=5000000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=n_actions, memory=memory, nb_steps_warmup=32,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['acc'])\n",
    "\n",
    "if os.path.isfile(path):\n",
    "    print(\"Carregando modelo...\")\n",
    "    dqn.load_weights(path)\n",
    "\n",
    "dqn.fit(env, nb_steps=2000, nb_max_episode_steps=100, visualize=False, verbose=2)\n",
    "\n",
    "dqn.save_weights(path, overwrite=True)\n",
    "\n",
    "# dqn.test(env, nb_episodes=5,nb_max_episode_steps=100, visualize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
