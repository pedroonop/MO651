{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trabalho1.ipynb","provenance":[],"collapsed_sections":["y67tHqDsYXIb","BvAkMvB8Ybj7","ExpKqXyShsML","K1nCWEI6lCfQ","yadl9LG0sew7","eOu-6EpnuaMz","ozy3LwXUzVBv"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OxHSR38zX0_i"},"source":["# MO651 - Robótica Móvel - Trabalho 01\n","### Equipe: \n","* Daiane Mendes - RA: 230210\n","* Carlos Victor - RA: 230261\n","* João Paulo - RA: 230221\n","* Pedro Olímpio - RA: 230256\n","\n","\n","## Introdução\n","\n","Este trabalho tem como objetivo utilizar um robô em um ambiente simulado para explorar e mapear um cenário complexo. Além disso, usaremos técnicas de odometria para estimar a posição do robô durante a exploração do ambiente.\n","\n","Usaremos o simulador v-rep e o robô Pioneer 3DX para execução dos experimentos. Com base nos resultados obtidos, realizaremos uma análise comparativa, verificando a qualidade dos mapeamentos feitos e das posições estimadas com odometria.\n","\n","O cenário explorado conta com diversas sala conectadas por portas e diversos obstáculos, como cadeiras, mesas, armários, pessoes, entre outros.\n","\n","O robô possui duas rodas independentes para locomoção e diversos sensores para obtenção de informações do ambiente, como uma câmera e sensores de proximidade.\n","\n","O controle do robô será feito obtendo informações do ambiente através dos sensores, planejando as ações com base nessas informações e agindo no ambiente através de atuadores, no caso, as rodas.\n","\n","<img src=\"imgs/fluxo.png\">"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WQH_AfnIcA1j"},"source":["## Metodologia\n","\n","Descreveremo as técnicas usadas para exploração do ambiente e cálculo da odometria."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y67tHqDsYXIb"},"source":["### Célula para o imports"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-Y505bsGX0_m","colab":{}},"source":["import sys, time, random, math, cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","sys.path.insert(0, '../src')\n","from robot import Robot"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BvAkMvB8Ybj7"},"source":["### Célula para as funções de processamento de imagens\n","\n","Utilizamos essas funções para encontrar portas no cenário utilizando a câmera. O objetivo de encontrar as portas é descobrir onde estão as salas adjacentes da sala atual no cenário para então acessá-las e mapea-las.\n","\n","A função *pre_processes_image* converte a imagem do formato fornecido pela câmera na simulação para um formato diferente. Utilizaremos essa imagem nesse novo formato para aplicar processamentos e funções da biblioteca *OpenCV*.\n","\n","A função *apply_mask_by_color* aplica um filtro que remove da imagem os objetos com cor diferente da cor da porta. Note que alteramos o cenário do experimento para as portas possuirem uma cor única. Essa função auxilia na detecção da posição das portas.\n","\n","A função *calculate_center* retorna a posição do contróide de uma porta na visão do robô. Caso não exista nenhuma porta na imagem obtida pela câmera, a função retorna uma tupla com valor falso no terceiro elemento. Caso existam mais de uma porta no campo de visão do robô, é retornado o centróide da porta de maior área na imagem. Utilizaremos essa função para encontrar portas."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HXuneC4JX0_q","colab":{}},"source":["def pre_process_image(resolution, image):\n","    img = np.array(image,dtype=np.uint8)\n","    img.resize([resolution[1],resolution[0],3])\n","    \n","    cv2.imwrite('vision'+'.png',cv2.flip(img, 0))\n","    \n","    return cv2.flip(img, 0)\n","\n","def apply_mask_by_color(image):\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    # define range of blue color in HSV\n","    lower_blue = np.array([110,50,50])\n","    upper_blue = np.array([130,255,255])\n","    \n","    # Threshold the HSV image to get only blue colors\n","    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n","    \n","    # Bitwise-AND mask and original image\n","    return cv2.bitwise_and(image,image, mask= mask)\n","\n","def calculate_center(image):\n","    cv2.imwrite('masked'+'.png',image)\n","    # convert image to grayscale image\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n","    # convert the grayscale image to binary image\n","    ret,thresh = cv2.threshold(gray_image,10,255, cv2.THRESH_BINARY)\n","    \n","    cv2.imwrite('thresh'+'.png',thresh)\n","    \n","    img, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n","    maximo = 0\n","    cmax = None\n","    for c in contours:\n","        area = cv2.contourArea(c)\n","        if area > maximo:\n","            maximo = area\n","            cmax = c\n","        \n","    # calculate moments for each contour\n","    M = cv2.moments(cmax)\n","    if(M[\"m00\"] != 0 ):\n","        # calculate x,y coordinate of center\n","        cX = int(M[\"m10\"] / M[\"m00\"])\n","        cY = int(M[\"m01\"] / M[\"m00\"])\n","\n","        cv2.circle(image, (cX, cY), 5, (255, 255, 255), -1)\n","        cv2.putText(image, \"center\", (cX - 25, cY - 25),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n","\n","        # display the image\n","        cv2.imwrite('door'+'.png',image)\n","        return cX, cY, True\n","    \n","    return 0, 0, False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ExpKqXyShsML"},"source":["### Célula para funções que mapeam o cenário\n","\n","Utilizamos essas funções para encontrar e armazenar os obstáculos existentes no cenário.\n","\n","A função *laser_sensors* utiliza os sensores de proximidade do robô para encontrar obstáculos próximos. Essa função também recebe como argumento a posição e orientação do robô e salva em uma imagem um mapa com a posição dos obstáculos encontrados.\n","\n","A função *update_map* calcula a posição atual do robô, usando o *Ground Truth*, e as medições dos sensores e usa a função *laser_sensors* para atualizar o mapa dos obstáculos."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OOcy3pUcX0_u","colab":{}},"source":["def laser_sensors(laser, c_position, gamma, label):\n","    x = []\n","    y = []\n","    \n","    rotation_matrix = np.matrix([[math.cos(gamma), -math.sin(gamma)], [math.sin(gamma), math.cos(gamma)]])\n","    for i in range(0, len(laser), 3):\n","        robot_position = np.matrix([[c_position[0]], [c_position[1]]])\n","        translation_matrix =  np.matrix([[laser[i]], [laser[i+1]]])\n","        T = np.matmul(rotation_matrix, translation_matrix)\n","        final_result = np.add(T, robot_position)\n","        x.append(final_result.item((0, 0)))\n","        y.append(final_result.item((1, 0)))\n","    plt.plot(x, y, label)\n","\n","def update_map_gt():\n","    c_position = robot.get_current_position()\n","    c_orientation = robot.get_current_orientation()[2]\n","    laser = robot.read_laser()\n","    plt.figure(1)\n","    print(c_orientation)\n","    laser_sensors(laser, c_position, c_orientation, 'b.')\n","    plt.savefig('map_gt'+'.png')\n","\n","def update_map_odometry(o_position, orientation):\n","    laser = robot.read_laser()\n","    plt.figure(2)\n","    print(orientation)\n","    laser_sensors(laser, o_position, orientation, 'r.')\n","    plt.savefig('map_odometry'+'.png')\n","\n","def update_map(o_position, orientation):\n","    update_map_gt()\n","    update_map_odometry(o_position, orientation)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K1nCWEI6lCfQ"},"source":["### Célula para funções que determinam o movimento do robô\n","\n","Essas funções definem a maneira que o robô vai se movimentar pelo ambiente. As regras para evitar colisão, evitar ficar preso e decisão da direção que o robô seguirá são definidas a seguir.\n","\n","A função *front_sensors_move* usa os 4 sensores frontais de proximidade para decidir a velocidade que será aplicada em cada roda do robô (esquerda e direita). Caso não existam obstáculos à frente do robô, a velocidade aplicada nas duas rodas será a mesma, ocasionando um movimento para frente. Caso exista algum obstáculo à frente do robô, será escolhida aleatóriamente uma direção que ele seguirá (esquerda ou direita). Caso exista algum obstáculo sendo detectado em alguma das direções, a direção que o robô seguirá não será aleatória, mas será aquela em que não exista obstáculo. Para isso, uma velocidade positiva será aplicada a uma das rodas e uma velocidade com valor oposto será aplicada na outra, ocasionando uma rotação do robô sobre o próprio eixo. Por fim, caso existam obstáculos na duas direções verificadas, aplicamos uma velocidade negativa em ambas as rodas, fazendo o robô se mover para trás.\n","\n","<img src=\"imgs/algo.png\">\n","\n","A função *walk* recebe a velocidade de cada roda e o tempo em segundos que o robô deverá manter essa velocidade. A função também é responsável por manter o robô em movimento com a velocidade e o tempo passados por parâmetro.\n","\n","Caso encontre uma porta, a função *enter_door* executa um procedimento para explorar a sala do outro lado da porta e sair. Para isso, quando uma porta é detectada pela câmera, o robô gira até estar alinhado com o centróide da porta. Em seguida, o robô se move em linha reta até ficar próximo à porta. Depois disso, o robô gira à direita até que seus sensores de proximidade frontais não detectem obstáculos próximos, ou seja, encontrou a passagem para a outra sala. Então, o robô avança para dentro da sala, vira à direita e à esquerda guardando as informações dos sensores sobre o ambiente e retorna de ré para a sala anterior.\n","\n","Para essa função de entrar na porta alteramos o código robot.py para definir como 10 metros a distância máxima de detecção dos sensores."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dSJSzjBZX0_z","scrolled":false,"colab":{}},"source":["def front_sensors_move(dist, vel):\n","    vLeft = vRight = defaultVelocity\n","    if (dist[3] < noDetectionDist or dist[4] < noDetectionDist):\n","        side = random.randint(0,1)\n","        if (dist[2] < noDetectionDistDiag and dist[5] > noDetectionDistDiag):\n","            side = 0\n","        elif (dist[5] < noDetectionDistDiag and dist[2] > noDetectionDistDiag):\n","            side = 1\n","        elif (dist[2] < noDetectionDistDiag and dist[5] < noDetectionDistDiag):\n","            distance = robot.read_ultrassonic_sensors()\n","            if (distance[2] < noDetectionDistDiag or distance[5] < noDetectionDistDiag or distance[1] < noDetectionDistDiag or distance[6] < noDetectionDistDiag):\n","                return[-defaultVelocity, -defaultVelocity]\n","        if (side != 0):\n","            distance = robot.read_ultrassonic_sensors()\n","            if (distance[3] < noDetectionDist or distance[4] < noDetectionDist):\n","                return [-(defaultVelocity)/2, defaultVelocity/2]\n","        else:\n","            distance = robot.read_ultrassonic_sensors()\n","            if (distance[3] < noDetectionDist or distance[4] < noDetectionDist):\n","                return [defaultVelocity/2, -defaultVelocity/2]\n","    return [vLeft, vRight]\n","\n","def walk(left_velocity, right_velocity, time_to_walk):\n","    robot.set_left_velocity(left_velocity)\n","    robot.set_right_velocity(right_velocity)  \n","    time.sleep(time_to_walk)\n","\n","\n","def enter_door(position, orientation):\n","    resolution, image = robot.read_vision_sensor()\n","    aux = resolution[0]/2\n","    image = pre_process_image(resolution, image)\n","    door = apply_mask_by_color(image)\n","    X,Y,isCenter = calculate_center(door)\n","    X += 5\n","    \n","    if (isCenter):\n","        while isCenter and abs(X - aux) > 5:\n","            if(X < aux):\n","                robot.set_left_velocity(-0.05)\n","                robot.set_right_velocity(0.05)\n","\n","            elif(X >= aux):\n","                robot.set_left_velocity(0.05)\n","                robot.set_right_velocity(-0.05)\n","                \n","            _, image = robot.read_vision_sensor()\n","            image = pre_process_image(resolution, image)\n","            door = apply_mask_by_color(image)\n","            X,Y,isCenter = calculate_center(door)\n","            X += 5\n","        if not isCenter:\n","            return\n","    \n","        us_distances = robot.read_ultrassonic_sensors()\n","        dist_front = min(us_distances[3], us_distances[4])\n","        while (dist_front > 0.3):\n","            robot.set_left_velocity(min(defaultVelocity, dist_front))\n","            robot.set_right_velocity(min(defaultVelocity, dist_front))\n","            us_distances = robot.read_ultrassonic_sensors()\n","            dist_front = min(us_distances[3], us_distances[4])\n","            \n","        while (dist_front < 7):\n","            robot.set_left_velocity(0.3)\n","            robot.set_right_velocity(-0.3)\n","            us_distances = robot.read_ultrassonic_sensors()\n","            dist_front = min(us_distances[3], us_distances[4])\n","        time.sleep(0.5)\n","        \n","        walk(defaultVelocity, defaultVelocity, 15)\n","        update_map(position, orientation)\n","        \n","        walk(1, -1, 4)\n","        update_map(position, orientation)\n","\n","        walk(-1, 1, 8)\n","        update_map(position, orientation)\n","        \n","        walk(1, -1, 4)\n","        \n","        walk(-defaultVelocity, -defaultVelocity, 20)\n","        walk(2, -2, 2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yadl9LG0sew7"},"source":["### Célula para o cálculo da Odometria\n","\n","Para esses cálculos alteramos o código *robot.py*, para adicionar a função *get_joint_position*."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BycEOUApfXqJ","colab":{}},"source":["def calculate_delta(begin, end, spinOrient):\n","    if math.isclose(float(end), float(begin), abs_tol=0.00001):\n","        return 0\n","    end = (limit + end) % limit\n","    begin = (limit + begin) % limit\n","\n","    ans = abs(end - begin)\n","    if spinOrient:\n","        if begin < end:\n","            return limit - ans\n","        return ans\n","    if begin > end:\n","        return limit - ans\n","    return ans\n","\n","def calculate_speed(time, clockwiseSpin, lastEncoder, motorHandle):\n","    encoder = robot.get_joint_position(motorHandle)\n","    delta = calculate_delta(lastEncoder, encoder, clockwiseSpin)\n","    \n","    lastEncoder = encoder\n","    if delta > math.pi:\n","        delta = (2 * math.pi) - delta\n","        clockwiseSpin = not clockwiseSpin\n","    \n","    speed = 0.0975 * (delta/time)\n","    if(clockwiseSpin):\n","        return -speed, clockwiseSpin, lastEncoder\n","    \n","    return speed, clockwiseSpin, lastEncoder\n","\n","def to180Universe(alpha):\n","    alpha = alpha % (2 * math.pi)\n","    if alpha > math.pi:\n","        return alpha - (2 * math.pi)\n","    return alpha\n","\n","def to360Universe(alpha):\n","    if alpha < 0:\n","        return (2 * math.pi) + alpha\n","    return alpha\n","\n","def add_angle(alpha, beta):\n","    angle = to360Universe(alpha) + to360Universe(beta)\n","    return to180Universe(angle)\n","\n","def new_position(alpha, x, y):\n","    global lastTime\n","    global lastEncoderL, lastEncoderR\n","    global clockwiseSpinL, clockwiseSpinR\n","    \n","    now = time.time()\n","    dtime = now-lastTime\n","    lastTime = now\n","    \n","    vL, clockwiseSpinL, lastEncoderL = calculate_speed(dtime, clockwiseSpinL, lastEncoderL, 'left')\n","    vR, clockwiseSpinR, lastEncoderR = calculate_speed(dtime, clockwiseSpinR, lastEncoderR, 'right')\n","    \n","    w = (vR-vL)/0.36205\n","    v = (vR + vL)/2.0\n","    \n","    dAlpha = w*dtime\n","    dL = vL*dtime\n","    dR = vR*dtime\n","    dM = (dL+dR)/2.0\n","    dX = dM*math.cos(add_angle(alpha, dAlpha/2.0))\n","    dY = dM*math.sin(add_angle(alpha, dAlpha/2.0))\n","    return x+dX, y+dY, add_angle(alpha, dAlpha)\n","\n","def plot_odometry_vs_GT(path, path_odometry):\n","    _, ax = plt.subplots(figsize=(9, 9))\n","    ax.plot(path_odometry[:,0], path_odometry[:,1], label = 'Odometry')\n","    ax.plot(path[:,0], path[:,1], label = 'Ground Truth')\n","    ax.grid(True)\n","    ax.legend(loc='best')\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"y\")\n","    plt.title('Odometry x Ground Truth')\n","    plt.savefig('odometry'+'.png') \n","#     plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eOu-6EpnuaMz"},"source":["### Célula do laço principal de controle\n","\n","Nesta célula, são definidas as variáveis que definirão como o robô irá se movimentar, ou seja, sua velocidade e sua distância de obstáculos. Isto é feito para planejar as ações do robô em diferentes circunstâncias.\n","\n","Variáveis para o cálculo da odometria também são definidas nesta célula. As informações de *Ground Truth* serão utilizadas como posição e orientação iniciais do robô para o cálculo da odometria de iterações seguintes.\n","\n","O laço principal será executado enquanto o robô estiver conectado. A cada iteração, usamos a função *front_sensors_move* para definir a velocidade que será aplicada em cada roda. Essas velocidades serão mantidas por 3 segundos. A cada iteração, a rotina de entrar em outra sala caso uma porta seja encontrada é chamada. Também a cada iteração é feito o cálculo de odometria para estimar a próxima posição."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FrlA5sInX0_4","outputId":"d97cb755-676e-4f56-febf-7e4da0640cf2","colab":{}},"source":["noDetectionDist = 0.7\n","defaultVelocity = 1\n","noDetectionDistDiag = 0.4\n","\n","robot = Robot()\n","\n","limit = 2*math.pi\n","lastTime=time.time()\n","clockwiseSpinL = False\n","clockwiseSpinR = False\n","lastEncoderL = robot.get_joint_position('left')\n","lastEncoderR = robot.get_joint_position('right')\n","\n","position = robot.get_current_position()\n","orientation = robot.get_current_orientation()\n","\n","alpha = orientation[2]\n","x = position[0]\n","y = position[1]\n","\n","\n","path = []\n","path_odometry = []\n","\n","while(robot.get_connection_status() != -1):\n","    \n","    position = robot.get_current_position()\n","    us_distances = robot.read_ultrassonic_sensors()\n","    vel = front_sensors_move(us_distances[:8], defaultVelocity)\n","    walk(vel[0], vel[1], 3)\n","    walk(0, 0, 0)\n","    \n","    update_map([x,y,0], alpha)\n","        \n","    enter_door([x,y,0], alpha)\n","    \n","    # Odometria\n","    x, y, alpha = new_position(alpha, x, y)\n","    path.append([-position[1], position[0]])\n","    path_odometry.append([-y, x])\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Connected to remoteApi server.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor1 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor2 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor3 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor4 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor5 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor6 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor7 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor8 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor9 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor10 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor11 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor12 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor13 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor14 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor15 connected.\n","\u001b[92m Pioneer_p3dx_ultrasonicSensor16 connected.\n","\u001b[92m Vision sensor connected.\n","\u001b[92m Laser connected.\n","\u001b[92m Left motor connected.\n","\u001b[92m Right motor connected.\n","\u001b[92m Robot connected.\n","-0.0001970789598999545\n","-5.952484207227826e-05\n","-0.00034695438807830215\n","0.00013240725978747037\n","-0.00040907642687670887\n","-6.005046665524816e-05\n","-0.0006218625348992646\n","-0.00040002166315744603\n","-0.0006715303752571344\n","-0.000378801553535979\n","-0.0007060252246446908\n","-0.0006713565595539706\n","-0.0008388885762542486\n","-0.000655850791101642\n","-0.000813699618447572\n","-0.0007433636788896081\n","-0.0009412615327164531\n","-0.0007299606595747576\n","-0.0009233892778865993\n","-0.0004503913333486764\n","-0.0009960291208699346\n","-0.0007791425172740674\n","-0.0008793917368166149\n","-0.0008000897510527238\n","-0.0007906418759375811\n","-0.0004206318150137278\n","-0.0008363930974155664\n","-0.0006906504867583152\n","-0.0007334978436119854\n","-0.0007029780542247011\n","-0.000670725479722023\n","-0.00036015771289843457\n","-0.0006387501489371061\n","-0.0003027815541685186\n","-0.000567046576179564\n","-0.00023497993310961363\n","-0.0005809853901155293\n","-0.00022624790615477508\n","-0.0007525159162469208\n","-0.00027889689220472746\n","-0.0008493132190778852\n","-0.0003226854391371603\n","-0.0008936375961638987\n","-0.0005876639335582468\n","-0.0009743831469677389\n","-0.0003836170095770086\n","-0.0010532846208661795\n","-0.0004237779129603325\n","-0.0012023303424939513\n","-0.0005773909606716643\n","0.6806426644325256\n","-0.0006640210148507464\n","1.3826643228530884\n","0.6915277846021066\n","1.4048223495483398\n","1.3633336299449104\n","1.4050618410110474\n","1.3619799421143588\n","1.405240774154663\n","1.3623475861315766\n","1.4053517580032349\n","1.3624415677819788\n","1.4053616523742676\n","1.3623387577955022\n","2.043923854827881\n","1.3626411844864033\n","1.7678176164627075\n","1.3626411844864033\n","0.07717520743608475\n","1.3626411844864033\n","-2.860987663269043\n","1.3626411844864033\n","0.3410559892654419\n","2.243889391403192\n","0.3408680260181427\n","1.9684264062560057\n","0.3405407667160034\n","1.9681108804559901\n","0.34029579162597656\n","1.9679312960486968\n","0.3399282395839691\n","1.9676419363638686\n","0.34035056829452515\n","1.9674053840429266\n","0.34030887484550476\n","1.967309203334267\n","0.3403201103210449\n","1.9674374549801616\n","0.46425139904022217\n","1.967302798777732\n","-0.07672534883022308\n","2.044962958525679\n","-0.1189902275800705\n","1.4994777291019785\n","-0.11899365484714508\n","1.4978216295633642\n","-0.11917015165090561\n","1.497657426525243\n","-0.11932042241096497\n","1.4975222366097691\n","0.5214537382125854\n","1.4974964899714696\n","-0.13679172098636627\n","2.1788711922196233\n","-0.7860460877418518\n","1.5088584621913235\n","-0.1691414713859558\n","0.8517175092982816\n","0.49737095832824707\n","1.5520647883866483\n","-0.11867158859968185\n","2.22597579059542\n","-0.8428043723106384\n","1.5719500856962085\n","-0.2092038244009018\n","0.8605116155069386\n","-0.8329598903656006\n","1.5610413441989426\n","-0.21030250191688538\n","0.8625011693248794\n","-0.826508104801178\n","1.562521647489017\n","-1.5858663320541382\n","0.8512860444369981\n","-0.9245136380195618\n","0.10882444118842916\n","-1.5942939519882202\n","0.7952260862812479\n","-2.353513717651367\n","0.08164600729246274\n","2.0455873012542725\n","0.08164600729246274\n","0.30783578753471375\n","0.08164600729246274\n","-2.635011911392212\n","0.08164600729246274\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ozy3LwXUzVBv"},"source":["###Célula para construir os gráficos de odometria"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WIqFqL6zfZOf","colab":{}},"source":["path = np.array(path)\n","path_odometry = np.array(path_odometry)\n","\n","difx = path[1,0] - path_odometry[1,0]\n","dify = path[1,1] - path_odometry[1,1]\n","path_odometry[:,0] = path_odometry[:,0]+difx\n","path_odometry[:,1] = path_odometry[:,1]+dify\n","\n","plot_odometry_vs_GT(path, path_odometry)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PhlYmrTAdtQd"},"source":["## Resultados\n","\n","Mostraremos os resultados obtidos neste trabalho.\n"]},{"cell_type":"markdown","metadata":{"id":"xf8jTw2utmo0","colab_type":"text"},"source":["A imagem a seguir mostra o cenário em que os testes foram realizados.\n","\n","<img src=\"imgs/scene.png\">"]},{"cell_type":"markdown","metadata":{"id":"qRsgPV35ta3U","colab_type":"text"},"source":["Usamos as posições e orientações do *Ground Truth* para criar um mapa do ambiente. Nas figuras exibidas a seguir os pontos representas um obstáculo qualquer, seja ele uma parede, perna de uma cadeira ou um sofá.\n","\n","Usando as técnicas descritas, de forma autônoma o robô mapeou grande parte do cenário. A imagem a seguir mostra o mapa obtido.\n","\n","<img src=\"imgs/mapa_gt_so.png\">"]},{"cell_type":"markdown","metadata":{"id":"79vlHdbPta3V","colab_type":"text"},"source":["Alterando manualmente a posição do robô conseguimos o mapeamento da figura a seguir:\n","\n","<img src=\"imgs/mapa_gt_mov.png\">\n","\n","Alterando apenas duas vezes a posição do robô conseguimos mapear quase por completo todo o cenário."]},{"cell_type":"markdown","metadata":{"id":"a2qupyPOta3V","colab_type":"text"},"source":["Geramos também mapas usando as posições estimadas do robô pela odometria e comparamos com os mapas gerados usando a posições do Ground Truth. As duas figuras a seguir mostram um mapa obtido usando as posições e orientações do Ground Truth (em azul) e da odometria (em vermelho).\n","\n","<img src=\"imgs/map_gt.png\">\n","\n","<img src=\"imgs/map_odometry.png\">\n","\n","Esses mapas foram feitos apenas com um simples movimento do robô, apenas avançar alguns metros. Portanto, as posições da odometria estão muito próxima das exatas e o mapa está com erros pequenos. O gráfico a seguir mostra a rota real seguida pelo robô e a rota estimada pela odometria para esse experimento.\n","\n","<img src=\"imgs/odometry1.png\">"]},{"cell_type":"markdown","metadata":{"id":"ZWzfoTuita3W","colab_type":"text"},"source":["Com a inclusão de curvas no percurso as posições estimadas pela odometria começam a distanciar das posições reais. Na construção dos mapas obstáculos foram redesenhados em diferentes posições e orientações, gerando mapas incorretos. As duas imagens a seguir mostram mapas obtidos com as posições reais obtidas pelo Ground Truth (em azul) e posições estimadas da odometria (em vermelho).\n","\n","<img src=\"imgs/map_gt2.png\">\n","\n","<img src=\"imgs/map_odometry2.png\">"]},{"cell_type":"markdown","metadata":{"id":"T5JBoopata3X","colab_type":"text"},"source":["A seguir o gráfico que mostra as rotas calculadas usando o Ground Truth e odometria.\n","\n","<img src=\"imgs/odometry2.png\">\n","\n","Apesar de distânciar da rota real, a rota calculada usando odometria ainda está muito semelhante a rota real, reproduzindo quase perfeitamente direção das curvas e comprimento das retas, falhando apenas em calcular o tamanho das curvas."]},{"cell_type":"markdown","metadata":{"id":"YVZAYMTFta3Y","colab_type":"text"},"source":["## Conclusões\n","\n","Nesse trabalho conseguimos mapear o ambiente quase por completo, guardando localização dos obstáculos de quase todos o cenário. Além disso, usando técnicas de odometria, conseguimos calcular a posição do robô com erros pequenos em relação a posição real."]}]}